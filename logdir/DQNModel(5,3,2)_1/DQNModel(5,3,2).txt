Model: DQNModel(5,3,2) 

Environment Variables 
depth_of_code = 5
rows = 3
cols = 2
max_swaps_per_time_step = -1

Model Variables 
learning_starts = 50000
verbose = 1
exploration_fraction = 0.5
exploration_initial_eps = 1
exploration_final_eps = 0.1
batch_size = 512
learning_rate = 0.0005
target_update_interval = 10000
tau = 0.5
gamma = 0.5
train_freq = 4

Training Variables 
total_timesteps = 1000000
log_interval = 4

Evaluation 
n_eval_episodes = 200

Results 
np.mean(rewards) = -51.825
mean_reward = -4.88
